# -*- coding: utf-8 -*-
"""reinforcement_learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zaAa3S3NavD1ZuR3psKCMXN47lcDd7sh
"""

import numpy as np
import pandas as pd
from collections import deque
import matplotlib.pyplot as plt


# Define the environment
class ImageContainerEnv:
    def __init__(self, data, max_high_risk_containers):
        self.data = data
        self.current_index = 0
        self.state = None
        self.action_space = [0, 1]  # 0 for flagging as high-risk, 1 for moving to the next container
        self.max_high_risk_containers = max_high_risk_containers
        self.filled_slot = 0

    def reset(self):
        self.current_index = 0
        self.state = self.get_state(self.current_index)
        self.filled_slot = 0
        return self.state

    def step(self, action):
        reward = self.get_reward(self.state, action)
        done = False
        selected = 0

        if action == 0:
            # Flag the current container as high-risk and move to the next
            self.current_index += 1
            selected += 1
            self.filled_slot += 1
        elif action == 1:
            # Move to the next container
            self.current_index += 1

        if self.current_index >= len(self.data) or self.filled_slot >= self.max_high_risk_containers:
            done = True

        next_state = self.get_state(self.current_index)
        self.state = next_state

        # Penalty for exceeding open slot limit
        penalty = 0
        if action == 0 and self.filled_slot > self.max_high_risk_containers:
            penalty = -0.1

        return next_state, reward + penalty, done, selected

    def get_state(self, index):
        if index >= len(self.data):
            return None
        state = np.append(self.data.iloc[index, :].values, self.max_high_risk_containers - self.filled_slot)
        return state

    def get_reward(self, state, action):
        if action == 0:
            payload = state[1]
            sharing_points = state[2]
            num_updates = state[3]

            a = 0.34
            b = 0.33
            c = 0.33

            reward = a * payload + b * sharing_points + c * (1 - num_updates)
            return reward
        else:
            return 0

# Define the Q-Learning agent
class QLearningAgent:
    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, epsilon=0.1, replay_capacity=1000):
        self.env = env
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.q_table = {}
        self.replay_buffer = deque(maxlen=replay_capacity)

    def get_q_value(self, state, action):
        state_tuple = tuple(state)
        if state_tuple not in self.q_table:
            self.q_table[state_tuple] = {0: 0, 1: 0}
        return self.q_table[state_tuple][action]

    def choose_action(self, state, remaining_slots):
        if np.random.rand() < self.epsilon:
            # Explore: Choose a random action
            action = np.random.choice(self.env.action_space)
        else:
            # Exploit: Choose the action with the highest Q-value
            q_values = [self.get_q_value(state, a) for a in self.env.action_space]
            max_q_value = max(q_values)
            max_actions = [a for a, q in zip(self.env.action_space, q_values) if q == max_q_value]

            if 0 in max_actions and remaining_slots > 0:
                # If flagging as high-risk is one of the max_actions and there are available slots, choose it
                action = 0
            else:
                # Otherwise, choose the action to move to the next container
                action = 1

        return action

    def train(self, num_episodes, max_steps, max_high_risk_containers):
        episode_rewards = []
        for episode in range(num_episodes):
            state = self.env.reset()
            total_reward = 0
            step = 0
            open_slot = max_high_risk_containers
            filled_slot = 0

            while True:
                action = self.choose_action(state, open_slot - filled_slot)  # Pass the available slot value
                next_state, reward, done, selected = self.env.step(action)
                self.replay_buffer.append((state, action, reward, next_state, done))
                self.update_q_value(self.replay_buffer)
                total_reward += reward
                state = next_state
                step += 1
                filled_slot += selected

                if done or step >= max_steps or filled_slot >= open_slot:
                    break

            episode_rewards.append(total_reward)
            print(f"Episode: {episode}, Total Reward: {total_reward}")

        # Plot the episode rewards
        plt.figure(figsize=(10, 6))
        plt.plot(episode_rewards)
        plt.xlabel('Episode')
        plt.ylabel('Total Reward')
        plt.title('Episode Rewards')

        plt.show()


    # Bellman's equation
    # Q(s,a) = Q(s,a) + α * (r + γ * max(Q(s',a')) - Q(s,a))
    def update_q_value(self, replay_buffer):
        for state, action, reward, next_state, done in replay_buffer:
            q_value = self.get_q_value(state, action)
            if next_state is not None:
                next_max_q = max([self.get_q_value(next_state, a) for a in self.env.action_space])
            else:
                next_max_q = 0
            new_q_value = q_value + self.learning_rate * (reward + self.discount_factor * next_max_q - q_value)
            state_tuple = tuple(state)
            self.q_table[state_tuple][action] = new_q_value

# Normalize the data
def normalize_data(data):
    # Normalize Payload
    data['Payload(gb)'] = (data['Payload(gb)'] - data['Payload(gb)'].min()) / (data['Payload(gb)'].max() - data['Payload(gb)'].min())

    # Normalize Sharing_Points
    data['Sharing_Points'] = (data['Sharing_Points'] - data['Sharing_Points'].min()) / (data['Sharing_Points'].max() - data['Sharing_Points'].min())

    # Normalize Num_Updates. Remember that smaller Num_Updates is better, so we invert the normalization.
    data['Num_Updates'] = 1 - (data['Num_Updates'] - data['Num_Updates'].min()) / (data['Num_Updates'].max() - data['Num_Updates'].min())

    return data

# Load the data
data = pd.read_csv("random_data.csv")

# Normalize the data
data = normalize_data(data)

# Create the environment
max_high_risk_containers = 500
env = ImageContainerEnv(data, max_high_risk_containers)

# Create the agent with a larger replay buffer
agent = QLearningAgent(env=env, replay_capacity=1000)

# Train the agent
num_episodes = 20
max_steps = len(data)

agent.train(num_episodes, max_steps, max_high_risk_containers)

# Evaluate the agent
container_rewards = []
state = env.reset()

while True:
    action = agent.choose_action(state, env.max_high_risk_containers - env.filled_slot)
    next_state, reward, done, selected = env.step(action)
    container_rewards.append((data.iloc[env.current_index - 1, 0], reward))  # Store the container name and reward
    state = next_state

    if done:
        break

# Sort the container rewards in descending order (highest reward first)
container_rewards.sort(key=lambda x: x[1], reverse=True)

# Create a new DataFrame with the high-risk containers
top_high_risk_containers = [container for container, reward in container_rewards[:max_high_risk_containers]]
top_high_risk_data = data[data['Image_Container'].isin(top_high_risk_containers)]

# Calculate the total reward for the top 500 high-risk containers
total_reward_top_500 = sum(reward for _, reward in container_rewards[:max_high_risk_containers])


# Save the top high-risk containers to a CSV file
top_high_risk_data.to_csv("top_high_risk_containers.csv", index=False)
print(f"Top {len(top_high_risk_data)} high-risk containers saved to 'top_high_risk_containers.csv'")
print(f"Total reward for the top 500 high-risk containers: {total_reward_top_500:.2f}")